# Mindful News - main.py v5.12

import feedparser
import openai
import json
import config
import os
import requests
import html
from bs4 import BeautifulSoup
from jinja2 import Environment, FileSystemLoader
from dateutil import parser as dateparser
from datetime import datetime, timezone
import re
import hashlib

# Version check
MAIN_VERSION = "2025-06-21-v5.12"

# Minimum characters required per article
MIN_CHARACTERS = 3000

# Detect BASE_DIR
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Load feeds
with open(os.path.join(BASE_DIR, "feeds.json"), encoding="utf-8") as f:
    feeds = json.load(f)

# Load prompts
def load_prompt(path):
    with open(os.path.join(BASE_DIR, path), "r", encoding="utf-8") as f:
        return f.read()

clustering_prompt_template = load_prompt("prompts/clustering_prompt.txt")
synthesis_prompt_template = load_prompt("prompts/synthesis_prompt.txt")

# Load rss_template.xml first line
with open(os.path.join(BASE_DIR, "templates/rss_template.xml"), encoding="utf-8") as f:
    rss_template_version = f.readline().strip()

# Setup OpenAI ‚Äî use env variable
openai.api_key = os.getenv("OPENAI_API_KEY")

# Helpers
def clean_xml_headers(text):
    return re.sub(r'\s*<\?xml[^>]+?\?>\s*', '', text, flags=re.IGNORECASE)

def strip_rss_tags(text):
    text = re.sub(r'<\/?rss[^>]*>', '', text, flags=re.IGNORECASE)
    text = re.sub(r'<\/?channel[^>]*>', '', text, flags=re.IGNORECASE)
    return text.strip()

def fetch_og_image(url):
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"):
                return og_image["content"]
            first_img = soup.find("img")
            if first_img and first_img.get("src"):
                return first_img.get("src")
    except Exception as e:
        print(f"‚ö†Ô∏è Error fetching image from {url}: {e}")
    return None

def filter_recent_articles(articles, now):
    recent = []
    for entry in articles:
        pub_date = entry.get("pubDate")
        if isinstance(pub_date, str):
            try:
                pub_date = dateparser.parse(pub_date)
            except Exception:
                pub_date = now
        if pub_date.tzinfo is None:
            pub_date = pub_date.replace(tzinfo=timezone.utc)
        age_hours = (now - pub_date).total_seconds() / 3600
        if age_hours <= config.RUN_INTERVAL_HOURS:
            recent.append(entry)
    return recent

def enforce_max_articles(articles):
    return articles[:config.MAX_ARTICLES]

def generate_synthesis(json_articles, max_attempts=3):
    prompt_body = synthesis_prompt_template + "\n\n" + json.dumps(json_articles, indent=2)
    attempts = 1
    while attempts <= max_attempts:
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt_body}],
            max_tokens=4000
        )
        output = response.choices[0].message.content.strip()
        output = clean_xml_headers(output)
        output = strip_rss_tags(output)
        if len(output) >= MIN_CHARACTERS or attempts == max_attempts:
            return output
        extra = (
            f"\n\nPlease expand the above synthesis to reach at least {MIN_CHARACTERS} characters, "
            "maintaining a calm and mindful tone."
        )
        prompt_body += extra
        attempts += 1
    return output

def main():
    now = datetime.now(timezone.utc)

    print(f"\nMindful News {MAIN_VERSION} ‚Äî version check:\n")
    print(f"main.py version: {MAIN_VERSION}")
    print(f"feeds.json version: unknown")
    print(f"clustering_prompt.txt version: {clustering_prompt_template.splitlines()[0]}")
    print(f"synthesis_prompt.txt version: {synthesis_prompt_template.splitlines()[0]}")
    print(f"rss_template.xml version: {rss_template_version}\n")

    # Fetch and parse feeds
    raw_articles = []
    for url in sum(feeds.values(), []):
        print(f"üåç Fetching feed: {url}")
        try:
            feed = feedparser.parse(url)
        except Exception as e:
            print(f"‚ö†Ô∏è Error fetching feed {url}: {e}")
            continue

        for entry in feed.entries:
            try:
                pub_date = dateparser.parse(entry.get("published", now.isoformat()))
            except Exception:
                pub_date = now
            if pub_date.tzinfo is None:
                pub_date = pub_date.replace(tzinfo=timezone.utc)
            age_hours = (now - pub_date).total_seconds() / 3600
            if age_hours > config.RUN_INTERVAL_HOURS:
                continue

            image_url = None
            if 'media_content' in entry:
                image_url = entry.media_content[0].get('url')
            elif 'media_thumbnail' in entry:
                image_url = entry.media_thumbnail[0].get('url')
            if not image_url:
                image_url = fetch_og_image(entry.link)

            raw_articles.append({
                "title": entry.title,
                "link": entry.link,
                "summary": clean_xml_headers(entry.get("summary", "")),
                "pubDate": pub_date,
                "image": image_url or ""
            })

    print(f"\n‚úÖ Total articles fetched: {len(raw_articles)}")

    # Deduplicate
    unique = {a['link']: a for a in raw_articles}
    articles = list(unique.values())
    print(f"‚úÖ Total articles after dedup: {len(articles)}")

    # Enforce maximum cap
    articles = enforce_max_articles(articles)
    print(f"‚úÖ Total articles after cap ({config.MAX_ARTICLES}): {len(articles)}")

    if not articles:
        print("‚ö†Ô∏è No articles to process ‚Äî exiting.")
        return

    # Second recency filter
    articles = filter_recent_articles(articles, now)

    # Prepare clustering
    titles = [a['title'] for a in articles]
    clustering_input = clustering_prompt_template + "\n\n" + json.dumps(titles, indent=2)
    print("\nüß† Calling OpenAI to cluster articles...")
    clustering_response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": clustering_input}],
        max_tokens=4000
    )
    clustering_output = clustering_response.choices[0].message.content
    try:
        json_start = clustering_output.find("[")
        json_end = clustering_output.rfind("]") + 1
        clustering_json = json.loads(clustering_output[json_start:json_end])
        print(f"‚úÖ Clustering done ‚Äî {len(clustering_json)} clusters found.\n")
    except Exception as e:
        print(f"‚ö†Ô∏è Error parsing clustering JSON: {e}")
        clustering_json = [{"theme": "General News", "articles": titles}]

    # Synthesis and build RSS items
    rss_items = []
    for cluster in clustering_json:
        selected = [a for a in articles if a["title"] in cluster.get("articles", [])]
        json_articles = [{
            "title": a["title"],
            "link": a["link"],
            "summary": a["summary"],
            "pubDate": a["pubDate"].isoformat(),
            "image": a["image"]
        } for a in selected]
        synthesis_output = generate_synthesis(json_articles)
        # Extract title from first line
        if synthesis_output.startswith("TITLE:"):
            lines = synthesis_output.splitlines()
            title_line = lines[0].replace("TITLE:", "").strip()
            synthesis_output = "\n".join(lines[1:]).strip()
        else:
            title_line = cluster.get("theme", "Untitled")
        # Clean HTML: keep only <p> tags
        soup = BeautifulSoup(synthesis_output, "html.parser")
        clean_html = "".join([str(p) for p in soup.find_all("p")])
        print(f"‚úÖ Synthesized article for theme: {cluster.get('theme')}")

        link = selected[0]["link"] if selected else ""
        image = selected[0]["image"] if selected else ""
        # Fallback image if blank or invalid
        default_image_url = "https://www.mindfulnews.media/wp-content/uploads/2025/06/ChatGPT-Image-Jun-18-2025-07_46_58-PM.png"  # <- replace with your logo or placeholder
        if not image or not image.startswith("http"):
            image = default_image_url

        pub_dates = [
            a["pubDate"] for a in selected if a.get("pubDate")
        ]
        if pub_dates:
            cluster_pubDate = max(pub_dates).strftime('%a, %d %b %Y %H:%M:%S +0000')
        else:
            cluster_pubDate = now.strftime('%a, %d %b %Y %H:%M:%S +0000')

        guid_source = f"{title_line}-{cluster_pubDate}-{link}"
        guid = hashlib.md5(guid_source.encode("utf-8")).hexdigest()

        rss_items.append({
            "title": html.escape(title_line),
            "link": link,
            "summary": clean_html,
            "pubDate": cluster_pubDate,
            "category": html.escape(cluster.get("theme", "")),
            "image": image,
            "positivity": "Constructive",
            "author": "Mindful News",
            "guid": guid,
        })

    # Render RSS
    env = Environment(loader=FileSystemLoader(os.path.join(BASE_DIR, "templates")))
    template = env.get_template("rss_template.xml")
    rss_content = template.render(
        articles=rss_items,
        build_date=now.strftime('%a, %d %b %Y %H:%M:%S +0000')
    )

    # Write RSS
    rss_output = os.path.join(BASE_DIR, config.OUTPUT_RSS_FILE)
    print(f"DEBUG: RSS output path resolved to: {rss_output}")
    if os.path.exists(rss_output):
        os.remove(rss_output)
        print(f"‚ÑπÔ∏è Previous {config.OUTPUT_RSS_FILE} removed.")
    with open(rss_output, "w", encoding="utf-8") as f:
        f.write(rss_content)

    print(f"\n‚úÖ Final RSS items: {len(rss_items)}")
    print(f"‚úÖ RSS feed written to {config.OUTPUT_RSS_FILE}")

if __name__ == "__main__":
    main()

